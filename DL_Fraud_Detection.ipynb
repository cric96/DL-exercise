{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Fraud-Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "JqOrnxrpYFy9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cric96/DL-exercise/blob/main/DL_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqOrnxrpYFy9"
      },
      "source": [
        "# **Autoencoders tutorial**\n",
        "In today's tutorial you will learn how to use autoencoders to solve the following tasks:\n",
        "- dimensionality reduction;\n",
        "- image denoising;\n",
        "- anomaly detection.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPpiRdjxqPO9"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code downloads all the necessary material into the remote machine. At the end of the execution select the **File** tab to verify that everything has been correctly downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-ByJkmYEk0"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1o8ZChPhCrI"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1DEyFzkdN9u"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF7IBOzqULRt"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **plot_2d_data** plots 2D labeled data;\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **show_confusion_matrix** visualizes a 2D confusion matrix as a color-coded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScuGEab6UM0d"
      },
      "source": [
        "def plot_2d_data(data_2d,y,titles=None,figsize=(7,7)):\n",
        "  _,axs=plt.subplots(1,len(data_2d),figsize=figsize)\n",
        "\n",
        "  for i in range(len(data_2d)):\n",
        "    if (titles!=None):\n",
        "      axs[i].set_title(titles[i])\n",
        "    scatter=axs[i].scatter(data_2d[i][:,0],data_2d[i][:,1],s=1,c=y[i],cmap=plt.cm.Paired)\n",
        "    axs[i].legend(*scatter.legend_elements())\n",
        "\n",
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def show_confusion_matrix(conf_matrix,class_names):\n",
        "  fig, ax = plt.subplots(figsize=(6,6))\n",
        "  img=ax.matshow(conf_matrix)\n",
        "  fig.colorbar(img)\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  _=plt.xticks(tick_marks, class_names,rotation=45)\n",
        "  _=plt.yticks(tick_marks, class_names)\n",
        "  _=plt.ylabel('Real')\n",
        "  _=plt.xlabel('Predicted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6puuu0XDAUe1"
      },
      "source": [
        "### **Model definition**\n",
        "The following function creates an undercomplete autoencoder given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the dimension of the latent space (*encoded_dim*);\n",
        "- the string identifier of the activation function of the hidden layers (*hidden_activation*);\n",
        "- the string identifier of the activation function of the output layer (*output_activation*).\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it isn't yet built);\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer.\n",
        "\n",
        "To combine encoder and decoder together forming the autoencoder, the [**Model**](https://keras.io/api/models/model/) class provided by Keras is used. Input and output layers are passed to the constructor, then it groups layers into an object with training and inference features.\n",
        "\n",
        "<u>Note that, the **build_autoencoder** function returns the encoder and the decoder models as well as the whole autoencoder.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kTIOHjjzMwW"
      },
      "source": [
        "def build_autoencoder(input_count, neuron_count_per_hidden_layer, encoded_dim, hidden_activation, output_activation):\n",
        "  #Encoder\n",
        "  encoder = keras.Sequential(name = 'encoder')\n",
        "  input_layer = layers.Input(shape = input_count, name = 'encoder_input');\n",
        "  encoder.add(input_layer)\n",
        "    \n",
        "  for neuron_count in neuron_count_per_hidden_layer:\n",
        "    hidden_layer = layers.Dense(neuron_count, activation = hidden_activation)\n",
        "    encoder.add(hidden_layer)\n",
        "      \n",
        "  latent_layer = layers.Dense(encoded_dim, activation = hidden_activation)\n",
        "  encoder.add(latent_layer)\n",
        "    \n",
        "  #Decoder\n",
        "  decoder = keras.Sequential(name = 'decoder')\n",
        "  decoder.add(layers.Input(shape = encoded_dim))\n",
        "  \n",
        "  for neuron_count in reversed(neuron_count_per_hidden_layer):\n",
        "    hidden_layer = layers.Dense(neuron_count, activation = hidden_activation)\n",
        "    decoder.add(hidden_layer)\n",
        "      \n",
        "  output_layer = layers.Dense(input_count, activation = output_activation)\n",
        "  decoder.add(output_layer)\n",
        "  \n",
        "  autoencoder = keras.Model(encoder.input, decoder(encoder.output), name = 'autoencoder')\n",
        "    \n",
        "  return autoencoder,encoder,decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZardAnq4Ehlq"
      },
      "source": [
        "# **Anomaly detection**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Mining activity was and is always connected with the occurrence of dangers which are commonly called\n",
        "mining hazards. A special case of such threat is a seismic hazard which frequently occurs in many\n",
        "underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards and in\n",
        "this respect it is comparable to an earthquake. More and more advanced seismic and seismoacoustic\n",
        "monitoring systems allow a better understanding rock mass processes and definition of seismic hazard\n",
        "prediction methods. Accuracy of so far created methods is however far from perfect. Complexity of\n",
        "seismic processes and big disproportion between the number of low-energy seismic events and the number\n",
        "of high-energy phenomena (e.g. > 10^4J) causes the statistical techniques to be insufficient to predict\n",
        "seismic hazard. Therefore, it is essential to search for new opportunities of better hazard prediction,\n",
        "also using machine learning methods. In seismic hazard assessment data clustering techniques can be\n",
        "applied (Lesniak A., Isakow Z.: Space-time clustering of seismic events and hazard assessment in the\n",
        "Zabrze-Bielszowice coal mine, Poland. Int. Journal of Rock Mechanics and Mining Sciences, 46(5), 2009,\n",
        "918-928), and for prediction of seismic tremors artificial neural networks are used (Kabiesz, J.: Effect\n",
        "of the form of data on the quality of mine tremors hazard forecasting using neural networks.\n",
        "Geotechnical and Geological Engineering, 24(5), 2005, 1131-1147). In the majority of applications, the\n",
        "results obtained by mentioned methods are reported in the form of two states which are interpreted as\n",
        "'hazardous' and 'non-hazardous'. Unbalanced distribution of positive ('hazardous state') and negative\n",
        "('non-hazardous state') examples is a serious problem in seismic hazard prediction. Currently used\n",
        "methods are still insufficient to achieve good sensitivity and specificity of predictions. In the paper\n",
        "(Bukowska M.: The probability of rockburst occurrence in the Upper Silesian Coal Basin area dependent on\n",
        "natural mining conditions. Journal of Mining Sciences, 42(6), 2006, 570-577) a number of factors having\n",
        "an effect on seismic hazard occurrence was proposed, among other factors, the occurrence of tremors with\n",
        "energy > 10^4J was listed. The task of seismic prediction can be defined in different ways, but the main\n",
        "aim of all seismic hazard assessment methods is to predict (with given precision relating to time and\n",
        "date) of increased seismic activity which can cause a rockburst. In the data set each row contains a\n",
        "summary statement about seismic activity in the rock mass within one shift (8 hours). If decision\n",
        "attribute has the value 1, then in the next shift any seismic bump with an energy higher than 10^4 J was\n",
        "registered. That task of hazards prediction bases on the relationship between the energy of recorded\n",
        "tremors and seismoacoustic activity with the possibility of rockburst occurrence. Hence, such hazard\n",
        "prognosis is not connected with accurate rockburst prediction. Moreover, with the information about the\n",
        "possibility of hazardous situation occurrence, an appropriate supervision service can reduce a risk of\n",
        "rockburst (e.g. by distressing shooting) or withdraw workers from the threatened area. Good prediction\n",
        "of increased seismic activity is therefore a matter of great practical importance. The presented data\n",
        "set is characterized by unbalanced distribution of positive and negative examples. In the data set there\n",
        "are only 170 positive examples representing class 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daa2iUhEm0k8"
      },
      "source": [
        "data = arff.loadarff('seismic-bumps.arff')\n",
        "dataframe = pd.DataFrame(data[0])\n",
        "ordinal_labels = [\"seismic\", \"seismoacoustic\", \"ghazard\", \"class\"]\n",
        "ordinal_encoding = OrdinalEncoder()\n",
        "dataframe[ordinal_labels] = ordinal_encoding.fit_transform(dataframe[ordinal_labels])\n",
        "dataframe[[\"shift_N\", \"shift_W\"]] = pd.get_dummies(dataframe['shift'], prefix='shift')\n",
        "dataframe = dataframe.drop(columns = [\"shift\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VYdGQU4HBwq"
      },
      "source": [
        "The variable *dataframe* is an instance of the pandas class [**DataFrame**](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html), a 2-dimensional labeled data structure with columns of potentially different types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc4wWBpJHKr6"
      },
      "source": [
        "### **Visualization**\n",
        "*row_count* randomly selected rows can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAbea04inJZ3"
      },
      "source": [
        "row_count = 5\n",
        "\n",
        "dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siHgJfXhHWza"
      },
      "source": [
        "### **Statistics**\n",
        "The [**info**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method can be used to print a brief summary of a **DataFrame** including the index and the type of each column, the non-null values and the memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOgV7HCYnMQM"
      },
      "source": [
        "dataframe.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BfFpFV3HwiR"
      },
      "source": [
        "To show the overall statistics of the dataset can be used the method [**describe**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv3-717OHpzr"
      },
      "source": [
        "dataframe.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obzgZWKYH_oa"
      },
      "source": [
        "The method [**hist**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html) draws a histogram for each column in the **DataFrame**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2PRZWrbIDHd"
      },
      "source": [
        "dataframe.hist(bins = 50, figsize = (20, 15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQbDBz6TIu6q"
      },
      "source": [
        "### **Split features from target values**\n",
        "The following code separates the features from the target values (clean/fraudulent transactions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUVmqIP_oFnU"
      },
      "source": [
        "dataframe_x = dataframe.drop(['class'], axis = 1)\n",
        "dataframe_y = dataframe['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkDxBvjeJXLq"
      },
      "source": [
        "The Numpy representation of a **DataFrame** can be obtained using the [**values**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3JJPnOkomRi"
      },
      "source": [
        "x = dataframe_x.values\n",
        "y = dataframe_y.values\n",
        "\n",
        "print('Feature shape: ',x.shape)\n",
        "print('Target shape: ',y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuVJ7YrcThat"
      },
      "source": [
        "cleans = y == 0\n",
        "frauds = y == 1\n",
        "\n",
        "clean_x = x[cleans]\n",
        "clean_y = y[cleans]\n",
        "\n",
        "fraud_x = x[frauds]\n",
        "fraud_y = y[frauds]\n",
        "\n",
        "print('Clean feature shape: ', clean_x.shape)\n",
        "print('Clean target shape: ', clean_y.shape)\n",
        "print('Fraudulent feature shape: ', fraud_x.shape)\n",
        "print('Fraudulent target shape: ', fraud_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGkNN8Rpo5T6"
      },
      "source": [
        "### **Split data into train, validation and test sets**\n",
        "In order to avoid overfitting during training and to evaluate the generalization capabilites of the models, it is necessary to divide the data into three disjoined datasets: training, validation and test sets.\n",
        "\n",
        "For this reason, the data are divided using the **train_test_split** function provided by Scikit-learn.\n",
        "\n",
        "The *test_size* and *val_size* parameters represent the percentage (or the absolute number) of patterns to include in the test and validation sets, respectively. \n",
        "\n",
        "<u>Note that, the autoencoder will be trained using only clean transactions while the test set will contain both clean and fraudulent transactions.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrfzU1-Zo-dq"
      },
      "source": [
        "test_size = 0.10\n",
        "val_size = 0.20\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(clean_x, clean_y, test_size = test_size,random_state = 1,shuffle=True)\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = val_size,random_state = 1,shuffle=True)\n",
        "\n",
        "test_x = np.concatenate((test_x, fraud_x))\n",
        "test_y = np.concatenate((test_y, fraud_y))\n",
        "\n",
        "print('Train feature shape: ',train_x.shape)\n",
        "print('Train target shape: ',train_y.shape)\n",
        "print('Validation feature shape: ',val_x.shape)\n",
        "print('Validation target shape: ',val_y.shape)\n",
        "print('Test feature shape: ',test_x.shape)\n",
        "print('Test target shape: ',test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlUJU0QPrAmC"
      },
      "source": [
        "### **Data normalization**\n",
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "Scikit-learn library provides the class [**StandardScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize features by removing the mean and scaling to unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raJty9W5o6Jd"
      },
      "source": [
        "scaler = StandardScaler().fit(train_x)\n",
        "train_x = scaler.transform(train_x)\n",
        "val_x = scaler.transform(val_x)\n",
        "test_x = scaler.transform(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgH1F4n-IM6w"
      },
      "source": [
        "pca = PCA(3)\n",
        "x_pca = pca.fit_transform(train_x)\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "x_pca.columns = ['PC1','PC2','PC3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-YH3uwnIeZr"
      },
      "source": [
        "norm = plt.Normalize()\n",
        "elements = train_y\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection = '3d')\n",
        "\n",
        "ax.scatter(x_pca.PC1, x_pca.PC2, x_pca.PC3, c = elements, alpha = 0.8)\n",
        "plt.title('Scatter plot')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSMdHAwRka-"
      },
      "source": [
        "## **The autoencoder**\n",
        "In this section an autoencoder is trained to detect anolies into credit card transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or452n77RVo3"
      },
      "source": [
        "### **Model creation**\n",
        "The following code creates the autoencoder by calling the **build_autoencoder** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1FQcPzrNtA"
      },
      "source": [
        "autoencoder, encoder, _ = build_autoencoder(input_count = train_x.shape[1], neuron_count_per_hidden_layer = [256, 256, 128, 128, 64, 64], encoded_dim = 4, hidden_activation = 'elu', output_activation = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpcJ3Er-SClG"
      },
      "source": [
        "### **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUqcbm1aSD82"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZcoMLOwSXCn"
      },
      "source": [
        "lternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jtPedgaSaUF"
      },
      "source": [
        "keras.utils.plot_model(autoencoder, show_shapes = True, show_layer_names = False, expand_nested = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9qApPfStPm"
      },
      "source": [
        "### **Model compilation**\n",
        "The following code compiles the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyZ-G3iVrTqj"
      },
      "source": [
        "autoencoder.compile(loss = 'mse', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWv5UTGwTBCe"
      },
      "source": [
        "### **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zea_QzWQrfwZ"
      },
      "source": [
        "epoch_count = 1000\n",
        "batch_size = 64\n",
        "patience = 50\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, restore_best_weights = True)\n",
        "\n",
        "history = autoencoder.fit(train_x, train_x, validation_data = (val_x, val_x), epochs = epoch_count, batch_size = batch_size, callbacks = [early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6cqF4EGUZEW"
      },
      "source": [
        "The following code calls the **plot_history** function defined above to draws in a graph the loss over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvGguf88Uabk"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7ok0AZruA-"
      },
      "source": [
        "## **Latent space visualization**\n",
        "It is always interesting to look at the compressed representation obtained by the autoencoder.\n",
        "\n",
        "The **predict** method of the *encoder* can be used to reduce training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH3hFuv1rvYA"
      },
      "source": [
        "encoded_train_x = encoder.predict(train_x)\n",
        "encoded_val_x = encoder.predict(val_x)\n",
        "encoded_test_x = encoder.predict(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFpkspPKUvHl"
      },
      "source": [
        "The following code visualize training,validation and test sets mapped into the latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngkyENylr9B-"
      },
      "source": [
        "plot_2d_data([encoded_train_x, encoded_val_x, encoded_test_x],[train_y, val_y, test_y],['Train','Validation','Test'],(15,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lIFVPKBdiJm"
      },
      "source": [
        "## **Anomaly detection**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BFBj9YfdZJf"
      },
      "source": [
        "reconstructed_train_x = autoencoder.predict(train_x)\n",
        "reconstructed_val_x = autoencoder.predict(val_x)\n",
        "reconstructed_test_x=autoencoder.predict(test_x)\n",
        "\n",
        "print('Reconstructed train shape: ',reconstructed_train_x.shape)\n",
        "print('Reconstructed validation shape: ',reconstructed_val_x.shape)\n",
        "print('Reconstructed test shape: ',reconstructed_test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNCTzBabH9C"
      },
      "source": [
        "Scikit-learn library provides the function [**mean_squared_error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) to compute MSE metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbJFfiY-dvwu"
      },
      "source": [
        "train_mse=mean_squared_error(train_x.transpose(),reconstructed_train_x.transpose(),multioutput='raw_values')\n",
        "val_mse=mean_squared_error(val_x.transpose(),reconstructed_val_x.transpose(),multioutput='raw_values')\n",
        "test_mse=mean_squared_error(test_x.transpose(),reconstructed_test_x.transpose(),multioutput='raw_values')\n",
        "\n",
        "print('Train MSE shape: ',train_mse.shape)\n",
        "print('Validation MSE shape: ',val_mse.shape)\n",
        "print('Test MSE shape: ',test_mse.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is04tIQhBtnE"
      },
      "source": [
        "### **Distribution of means squared error**\n",
        "The following code draws the MSE distributions of training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvEGptTTejqn"
      },
      "source": [
        "_, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "\n",
        "axs[0].hist(train_mse, bins=100, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
        "axs[0].set_title('Train')\n",
        "\n",
        "axs[1].hist(val_mse, bins=100, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
        "axs[1].set_title('Validation')\n",
        "\n",
        "axs[2].hist(test_mse[(test_y==0).squeeze()], bins=100, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
        "axs[2].hist(test_mse[(test_y==1).squeeze()], bins=100, density=True, label=\"fraudulent\", alpha=.6, color=\"red\")\n",
        "axs[2].set_title('Test')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj8wiaz0cYic"
      },
      "source": [
        "Looking at the test distribution, although some fraudulent transactions present a low MSE very similar to clean transactions, in general the fraudulent transactions clearly have a distinguishing element in their data that sets them apart from clean ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD_SY7-XDGNF"
      },
      "source": [
        "### **Detection accuracy**\n",
        "To detect fraudulent transactions a threshold on the MSE value can be used. \n",
        "\n",
        "It must be chosen to limit as much as possible the amount of clean transactions classified as fraudulent (i.e., false positive) and to capture the most anomalous ones.\n",
        "\n",
        "Here we select as threshold the MSE value to obtain a specific percentage of true negatives on the validation set. \n",
        "\n",
        "The MSE value corresponding to a specific percentage (*clean_acceptance_rate*) of true negatives (i.e., clean transactions correctly classified) on the validation set is chosen as threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTj4a1LEDRql"
      },
      "source": [
        "clean_acceptance_rate = 0.90\n",
        "\n",
        "sorted_val_mse=np.sort(val_mse)\n",
        "\n",
        "idx=int(clean_acceptance_rate*len(sorted_val_mse))\n",
        "\n",
        "thr=sorted_val_mse[int(clean_acceptance_rate*len(sorted_val_mse))]\n",
        "\n",
        "print('Anomaly detection threshold: {:.3f}'.format(thr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bohGKXmk3xV"
      },
      "source": [
        "The accuracy can be easily measured by calling the [**accuracy_score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) method provided by the Scikit-learn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz1KyNPSiyrm"
      },
      "source": [
        "train_y_pred=train_mse > thr\n",
        "val_y_pred=val_mse > thr\n",
        "test_y_pred=test_mse > thr\n",
        "\n",
        "train_accuracy = accuracy_score(train_y,train_y_pred,normalize='true')\n",
        "val_accuracy = accuracy_score(val_y,val_y_pred,normalize='true')\n",
        "test_accuracy = accuracy_score(test_y,test_y_pred,normalize='true')\n",
        "\n",
        "print('Train accuracy: {:.3f}'.format(train_accuracy))\n",
        "print('Validation accuracy: {:.3f}'.format(val_accuracy))\n",
        "print('Test accuracy: {:.3f}'.format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ0KqnvZlmvh"
      },
      "source": [
        "### **Confusion matrix**\n",
        "To evaluate the classification accuracy in presence of an unbalanced dataset, it is useful to compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "\n",
        "Scikit-learn library provides the function [**confusion_matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compute the confusion matrix given the grouhd truth (*test_y*) and the predicted classes (*test_y_pred*) as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkG6N3mBGS4Q"
      },
      "source": [
        "conf_matrix=confusion_matrix(test_y, test_y_pred, normalize='true')\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZOQTm-onPxu"
      },
      "source": [
        "The following code visualizes the 2D confusion matrix as a color-coded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDIVaNwinSrd"
      },
      "source": [
        "show_confusion_matrix(conf_matrix,('normal','problem'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}