{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-with-rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH3UCD6srRsuqWsS+SqghR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cric96/DL-exercise/blob/main/test_with_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2Yp08lPMHJ"
      },
      "source": [
        "# Recurrent Neural Network applied in Aggregate Computing\n",
        "In this notebook, I tried to apply Recurrent Neural Networks (RNN) in the context of Aggregate Computing (AC).\n",
        "\n",
        "## Model\n",
        "\n",
        "Usually, RNN are trained in indipendent sequences. In case of Aggregate Computing, the temporal sequence are correlated with each other following a neighbourhood policy. \n",
        "\n",
        "The key idea here is:\n",
        "- express the system as a graph. Offline we can imagine to access to the entire system\n",
        "- in each time step, we aggregate data from the neighbours and then we use RNN to compute the right output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihJbe7wP_L7"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld4IkeCHdg9f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorboard\n",
        "from datetime import datetime\n",
        "from numpy.random import seed\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCXGw5707Rv7"
      },
      "source": [
        "# Simple Graph\n",
        "\n",
        "In this model, I model graph as:\n",
        "$G = (N, E)$\n",
        "\n",
        "Where, $N$ contains a feature vector and an output vector.\n",
        "\n",
        "$E$ is expressed as adjacency matrix, so: $E_{i,j} = 1$ iff $i$ is neighbour of $j$.\n",
        "\n",
        "Input is a matrix contains the concatenation of feature vector and output vector, something like: $ I(n) = <f(n), o(n)>$.\n",
        "Where $<a, b, c>$ means the column-wise fector concatenation. \n",
        "For instance, giving a vector $a = [1, 2]$ and $b = [2, 3]$, $<a,b> = [1, 2, 2, 3]$.\n",
        "\n",
        "The neighbour aggregation data can be computed leveraging matrix multiplication --- so increasing traning performance.\n",
        "$$ reduction(E * F) $$\n",
        "Where F contains all node features.\n",
        "$reduction$ is a multiset function (e.g. summation, mean,...).\n",
        "\n",
        "Pay attention, this model can be easly used in a decentralized situation.\n",
        "Indeed, the data aggregation will be done without global adjency matrix but only retrieving neighbour data. The dense layer already works locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaicADIO7U34"
      },
      "source": [
        "n = 50000\n",
        "feature = tf.constant([\n",
        "                     [1], \n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0],\n",
        "                    ], dtype=tf.float32)\n",
        "output = tf.constant([\n",
        "                     [0], \n",
        "                     [n],\n",
        "                     [n],\n",
        "                     [n],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "input = tf.concat([feature, output], axis = 1)\n",
        "input = input[:,tf.newaxis, :]\n",
        "\n",
        "neigh = tf.constant([\n",
        "                     [n, 1, n, n], \n",
        "                     [1, n, 1, n],\n",
        "                     [n, 1, n, 1],\n",
        "                     [n, n, 1, n],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "ground = tf.constant([\n",
        "                     [1, 0], \n",
        "                     [0, 1],\n",
        "                     [0, 2],\n",
        "                     [0, 3],\n",
        "                    ], dtype=tf.float32)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRY-rUZ27Y0E"
      },
      "source": [
        "# Forward Pass\n",
        "\n",
        "Taking the output of previous step, adjecency matrix, and feature vector, this function compute the value of the next evaluation.\n",
        "So\n",
        "1. compute neighbourhood feature via aggregation\n",
        "2. concat neighbourhood feature with local feature and previous output\n",
        "3. perform a forward pass of a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKorlgUN7arq"
      },
      "source": [
        "def forward(result, neigh, feature, eval_logic, log_enable=False):\n",
        "  if(log_enable):\n",
        "    print(\"Input = \", result)\n",
        "  reshape = tf.reshape(result[:,:, 1], feature.shape[0])\n",
        "  neigh_evaluation = tf.reduce_min(tf.multiply(neigh, reshape), 1) ## pass reduction strategy\n",
        "  input_network = tf.concat([result[:,:, 0], neigh_evaluation[:, tf.newaxis]], 1)\n",
        "  if(log_enable):\n",
        "    print(\"Neighbour Aggregation = \", input_network[:, tf.newaxis])\n",
        "  result = eval_logic(input_network[:, tf.newaxis])\n",
        "  result = tf.concat([feature, result], axis = 1)\n",
        "  if(log_enable):\n",
        "    print(\"Output = \", result)\n",
        "  return tf.expand_dims(result, [1])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aJqP660S5W4"
      },
      "source": [
        "# Model creation\n",
        "Create a sequential model given layers and the input shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdXR3Tb3TV8l"
      },
      "source": [
        "def create_model(input_shape, layers):\n",
        "  input_layer = tf.keras.layers.InputLayer(input_shape=input.shape[1:], batch_size=input_shape[0])\n",
        "  layers.insert(0, input_layer)\n",
        "  return tf.keras.Sequential(layers)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOdTAPsnGEf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oNvEE-Bh4B1"
      },
      "source": [
        "def instantiate_layers():\n",
        "  return [\n",
        "      tf.keras.layers.LSTM(units = 4, activation='relu', return_sequences=True, stateful=True),\n",
        "      tf.keras.layers.LSTM(units = 1, activation='relu', return_sequences=False, stateful=True, bias_initializer='ones'),\n",
        "  ]\n",
        "model = create_model(input.shape, instantiate_layers())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZquNuutUD-n"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvzeO1W3UGi4"
      },
      "source": [
        "## TODO pass input and network\n",
        "def train(model, data, iteration, stabilise_in, stabilisation_check, loss, optimizer, each=100):\n",
        "  input, neigh, feature, ground = data\n",
        "  for j in range(iteration):\n",
        "    with tf.GradientTape() as tape:\n",
        "      result = input\n",
        "      to_backprop = 0\n",
        "      for i in range(stabilise_in):\n",
        "        result = forward(result, neigh, feature, model)\n",
        "      for i in range(stabilisation_check):\n",
        "        result = forward(result, neigh, feature, model)\n",
        "        to_backprop += 1 / stabilisation_check * loss(ground, result[:, 0, :])\n",
        "      model.reset_states()\n",
        "      gradient = tape.gradient(to_backprop, model.weights)\n",
        "      optimizer.apply_gradients(zip(gradient, model.weights))\n",
        "    if(j % each == 0):\n",
        "      print(\"Epoch \", j ,\"Loss = \", tf.reduce_sum(to_backprop))  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFGPPgD_hdHi"
      },
      "source": [
        "\n",
        "## Train loop\n",
        "Here I want only to find a function that overfits, so it solve this specific problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "56wyzp3MdzYy",
        "outputId": "836071bc-5325-4b6b-dfe7-42886ea71aaa"
      },
      "source": [
        "#seed(42)\n",
        "#tf.random.set_seed(42)\n",
        "model = create_model(input.shape, instantiate_layers()) ## comment to avoid the recomputation of weights\n",
        "iteration = 10000\n",
        "stabilise_in = 4\n",
        "stabilisation_check = 5\n",
        "\n",
        "loss = tf.losses.mse\n",
        "optimizer = tf.optimizers.Adam()\n",
        "data = (input, neigh, feature, ground)\n",
        "train(model, data, iteration, stabilise_in, stabilisation_check, loss, optimizer, 50)\n",
        "\n",
        "result = input\n",
        "for i in range(20):\n",
        "  result = forward(result, neigh, feature, model)\n",
        "print(result)\n",
        "model.reset_states()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a683f71e4198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstabilise_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstabilisation_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9d31d716c52e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, iteration, stabilise_in, stabilisation_check, loss, optimizer, each)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meach\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"Loss = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_backprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m---> 76\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     logging.warning(\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['lstm_39/lstm_cell_39/kernel:0', 'lstm_39/lstm_cell_39/recurrent_kernel:0', 'lstm_39/lstm_cell_39/bias:0', 'lstm_40/lstm_cell_40/kernel:0', 'lstm_40/lstm_cell_40/recurrent_kernel:0', 'lstm_40/lstm_cell_40/bias:0']."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqbsEJEnWOc7"
      },
      "source": [
        "# Check result, generalisation\n",
        "In this part, I try to use the same network in another graph, to see if it can be used in different graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LDTXF5knmGv",
        "outputId": "dce11d28-24ab-4b1f-de63-6fd53a9ddc21"
      },
      "source": [
        "n = 50000\n",
        "feature_validation = tf.constant([\n",
        "                     [0], \n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0]\n",
        "                    ], dtype=tf.float32)\n",
        "output_validation = tf.constant([\n",
        "                     [0], \n",
        "                     [n],\n",
        "                     [n],\n",
        "                     [n],\n",
        "                     [n]\n",
        "                    ], dtype=tf.float32)\n",
        "input_validation = tf.concat([feature_validation, output_validation], axis = 1)\n",
        "input_validation = input_validation[:,tf.newaxis, :]\n",
        "neigh_validation = tf.constant([\n",
        "                     [n, 1, n, n, n], \n",
        "                     [1, n, 1, n, n],\n",
        "                     [n, 1, n, 1, n],\n",
        "                     [n, n, 1, n, 1],\n",
        "                     [n, n, n, 1, n],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "change_model = create_model(input_validation.shape, instantiate_layers())\n",
        "change_model.set_weights(model.get_weights())\n",
        "\n",
        "result_validation = input_validation\n",
        "model.reset_states()\n",
        "for i in range(20):\n",
        "  result_validation = forward(result_validation, neigh_validation, feature_validation, change_model, False)\n",
        "print(result_validation)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0.0000000e+00 1.2036740e+00]]\n",
            "\n",
            " [[0.0000000e+00 1.2035980e+00]]\n",
            "\n",
            " [[1.0000000e+00 1.1088027e-27]]\n",
            "\n",
            " [[0.0000000e+00 1.2035980e+00]]\n",
            "\n",
            " [[0.0000000e+00 1.2036740e+00]]], shape=(5, 1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}