{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-with-rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0eOXyiQxfa//n8z5T/iN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cric96/DL-exercise/blob/main/test_with_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2Yp08lPMHJ"
      },
      "source": [
        "# Recurrent Neural Network applied in Aggregate Computing\n",
        "In this notebook, I tried to apply Recurrent Neural Networks (RNN) in the context of Aggregate Computing (AC).\n",
        "\n",
        "## Model\n",
        "\n",
        "Usually, RNN are trained in indipendent sequences. In case of Aggregate Computing, the temporal sequence are correlated with each other following a neighbourhood policy. \n",
        "\n",
        "The key idea here is:\n",
        "- express the system as a graph. Offline we can imagine to access to the entire system\n",
        "- in each time step, we aggregate data from the neighbours and then we use RNN to compute the right output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihJbe7wP_L7"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld4IkeCHdg9f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorboard\n",
        "from datetime import datetime\n",
        "from numpy.random import seed\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCXGw5707Rv7"
      },
      "source": [
        "# Simple Graph\n",
        "\n",
        "In this model, I model graph as:\n",
        "$G = (N, E)$\n",
        "\n",
        "Where, $N$ contains a feature vector and an output vector.\n",
        "\n",
        "$E$ is expressed as adjacency matrix, so: $E_{i,j} = 1$ iff $i$ is neighbour of $j$.\n",
        "\n",
        "Input is a matrix contains the concatenation of feature vector and output vector, something like: $ I(n) = <f(n), o(n)>$.\n",
        "Where $<a, b, c>$ means the column-wise fector concatenation. \n",
        "For instance, giving a vector $a = [1, 2]$ and $b = [2, 3]$, $<a,b> = [1, 2, 2, 3]$.\n",
        "\n",
        "The neighbour aggregation data can be computed leveraging matrix multiplication --- so increasing traning performance.\n",
        "$$ reduction(E * F) $$\n",
        "Where F contains all node features.\n",
        "$reduction$ is a multiset function (e.g. summation, mean,...).\n",
        "\n",
        "Pay attention, this model can be easly used in a decentralized situation.\n",
        "Indeed, the data aggregation will be done without global adjency matrix but only retrieving neighbour data. The dense layer already works locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaicADIO7U34"
      },
      "source": [
        "feature = tf.constant([\n",
        "                     [1], \n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0],\n",
        "                    ], dtype=tf.float32)\n",
        "output = tf.constant([\n",
        "                     [100.0], \n",
        "                     [100.0],\n",
        "                     [100.0],\n",
        "                     [100.0],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "input = tf.concat([feature, output], axis = 1)\n",
        "input = input[:,tf.newaxis, :]\n",
        "\n",
        "n = 10000\n",
        "neigh = tf.constant([\n",
        "                     [n, 1, n, n], \n",
        "                     [1, n, 1, n],\n",
        "                     [n, 1, n, 1],\n",
        "                     [n, n, 1, n],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "ground = tf.constant([\n",
        "                     [1, 0], \n",
        "                     [0, 1],\n",
        "                     [0, 2],\n",
        "                     [0, 3],\n",
        "                    ], dtype=tf.float32)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRY-rUZ27Y0E"
      },
      "source": [
        "# Forward Pass\n",
        "\n",
        "Taking the output of previous step, adjecency matrix, and feature vector, this function compute the value of the next evaluation.\n",
        "So\n",
        "1. compute neighbourhood feature via aggregation\n",
        "2. concat neighbourhood feature with local feature and previous output\n",
        "3. perform a forward pass of a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKorlgUN7arq"
      },
      "source": [
        "def forward(result, neigh, feature, eval_logic, log_enable=False):\n",
        "  if(log_enable):\n",
        "    print(\"Input = \", result)\n",
        "  reshape = tf.reshape(result[:,:, 1], feature.shape[0])\n",
        "  neigh_evaluation = tf.reduce_min(tf.multiply(neigh, reshape), 1) ## pass reduction strategy\n",
        "  input_network = tf.concat([result[:,:, 0], neigh_evaluation[:, tf.newaxis]], 1)\n",
        "  if(log_enable):\n",
        "    print(\"Neighbour Aggregation = \", input_network[:, tf.newaxis])\n",
        "  result = eval_logic(input_network[:, tf.newaxis])\n",
        "  result = tf.concat([feature, result], axis = 1)\n",
        "  if(log_enable):\n",
        "    print(\"Output = \", result)\n",
        "  return tf.expand_dims(result, [1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aJqP660S5W4"
      },
      "source": [
        "# Model creation\n",
        "Utilise sequential model to combine rnn with dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdXR3Tb3TV8l"
      },
      "source": [
        "def create_model(input_shape, layers):\n",
        "  input_layer = tf.keras.layers.InputLayer(input_shape=input.shape[1:], batch_size=input_shape[0])\n",
        "  layers.insert(0, input_layer)\n",
        "  return tf.keras.Sequential(layers)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oNvEE-Bh4B1"
      },
      "source": [
        "def instantiate_layers():\n",
        "  return [\n",
        "      tf.keras.layers.LSTM(units = 4, activation='relu', return_sequences=True, stateful=True),\n",
        "      tf.keras.layers.GRU(units = 1, activation='relu', return_sequences=False, stateful=True),\n",
        "  ]\n",
        "model = create_model(input.shape, instantiate_layers())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZquNuutUD-n"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvzeO1W3UGi4"
      },
      "source": [
        "## TODO pass input and network\n",
        "def train(model, data, iteration, stabilise_in, stabilisation_check, loss, optimizer, each=100):\n",
        "  input, neigh, feature, ground = data\n",
        "  for j in range(iteration):\n",
        "    with tf.GradientTape() as tape:\n",
        "      result = input\n",
        "      to_backprop = 0\n",
        "      for i in range(stabilise_in):\n",
        "        result = forward(result, neigh, feature, model)\n",
        "      for i in range(stabilisation_check):\n",
        "        result = forward(result, neigh, feature, model)\n",
        "        to_backprop += 1 / stabilisation_check * loss(ground, result[:, 0, :])\n",
        "      model.reset_states()\n",
        "      gradient = tape.gradient(to_backprop, model.weights)\n",
        "      optimizer.apply_gradients(zip(gradient, model.weights))\n",
        "    if(j % each == 0):\n",
        "      print(\"Epoch \", j ,\"Loss = \", tf.reduce_sum(to_backprop))  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFGPPgD_hdHi"
      },
      "source": [
        "\n",
        "## RNN With Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56wyzp3MdzYy",
        "outputId": "104d5e20-2fc4-496d-d28e-ff510e74e438"
      },
      "source": [
        "#seed(42)\n",
        "#tf.random.set_seed(42)\n",
        "model = create_model(input.shape, instantiate_layers()) ## comment to avoid the recomputation of weights\n",
        "iteration = 10000\n",
        "stabilise_in = 4\n",
        "stabilisation_check = 5\n",
        "\n",
        "loss = tf.losses.mse\n",
        "optimizer = tf.optimizers.Adam()\n",
        "data = (input, neigh, feature, ground)\n",
        "train(model, data, iteration, stabilise_in, stabilisation_check, loss, optimizer, 50)\n",
        "\n",
        "result = input\n",
        "for i in range(20):\n",
        "  result = forward(result, neigh, feature, model)\n",
        "print(result)\n",
        "model.reset_states()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 Loss =  tf.Tensor(6.5195093, shape=(), dtype=float32)\n",
            "Epoch  50 Loss =  tf.Tensor(0.89710075, shape=(), dtype=float32)\n",
            "Epoch  100 Loss =  tf.Tensor(0.61277646, shape=(), dtype=float32)\n",
            "Epoch  150 Loss =  tf.Tensor(0.44200557, shape=(), dtype=float32)\n",
            "Epoch  200 Loss =  tf.Tensor(1.2568748, shape=(), dtype=float32)\n",
            "Epoch  250 Loss =  tf.Tensor(0.84539706, shape=(), dtype=float32)\n",
            "Epoch  300 Loss =  tf.Tensor(0.8289993, shape=(), dtype=float32)\n",
            "Epoch  350 Loss =  tf.Tensor(0.8808413, shape=(), dtype=float32)\n",
            "Epoch  400 Loss =  tf.Tensor(0.5887662, shape=(), dtype=float32)\n",
            "Epoch  450 Loss =  tf.Tensor(0.51145154, shape=(), dtype=float32)\n",
            "Epoch  500 Loss =  tf.Tensor(0.40449935, shape=(), dtype=float32)\n",
            "Epoch  550 Loss =  tf.Tensor(0.4726902, shape=(), dtype=float32)\n",
            "Epoch  600 Loss =  tf.Tensor(0.540628, shape=(), dtype=float32)\n",
            "Epoch  650 Loss =  tf.Tensor(0.20738217, shape=(), dtype=float32)\n",
            "Epoch  700 Loss =  tf.Tensor(0.20120652, shape=(), dtype=float32)\n",
            "Epoch  750 Loss =  tf.Tensor(0.18750697, shape=(), dtype=float32)\n",
            "Epoch  800 Loss =  tf.Tensor(0.16060397, shape=(), dtype=float32)\n",
            "Epoch  850 Loss =  tf.Tensor(0.12287013, shape=(), dtype=float32)\n",
            "Epoch  900 Loss =  tf.Tensor(0.09858405, shape=(), dtype=float32)\n",
            "Epoch  950 Loss =  tf.Tensor(0.17472461, shape=(), dtype=float32)\n",
            "Epoch  1000 Loss =  tf.Tensor(0.17936133, shape=(), dtype=float32)\n",
            "Epoch  1050 Loss =  tf.Tensor(0.055773944, shape=(), dtype=float32)\n",
            "Epoch  1100 Loss =  tf.Tensor(0.047974434, shape=(), dtype=float32)\n",
            "Epoch  1150 Loss =  tf.Tensor(0.045931958, shape=(), dtype=float32)\n",
            "Epoch  1200 Loss =  tf.Tensor(0.046905763, shape=(), dtype=float32)\n",
            "Epoch  1250 Loss =  tf.Tensor(0.050742418, shape=(), dtype=float32)\n",
            "Epoch  1300 Loss =  tf.Tensor(0.057995662, shape=(), dtype=float32)\n",
            "Epoch  1350 Loss =  tf.Tensor(0.072325364, shape=(), dtype=float32)\n",
            "Epoch  1400 Loss =  tf.Tensor(0.08842002, shape=(), dtype=float32)\n",
            "Epoch  1450 Loss =  tf.Tensor(0.123632014, shape=(), dtype=float32)\n",
            "Epoch  1500 Loss =  tf.Tensor(0.106341764, shape=(), dtype=float32)\n",
            "Epoch  1550 Loss =  tf.Tensor(0.0894266, shape=(), dtype=float32)\n",
            "Epoch  1600 Loss =  tf.Tensor(0.076046236, shape=(), dtype=float32)\n",
            "Epoch  1650 Loss =  tf.Tensor(0.06911372, shape=(), dtype=float32)\n",
            "Epoch  1700 Loss =  tf.Tensor(0.058443747, shape=(), dtype=float32)\n",
            "Epoch  1750 Loss =  tf.Tensor(0.053929932, shape=(), dtype=float32)\n",
            "Epoch  1800 Loss =  tf.Tensor(0.07873763, shape=(), dtype=float32)\n",
            "Epoch  1850 Loss =  tf.Tensor(0.045007024, shape=(), dtype=float32)\n",
            "Epoch  1900 Loss =  tf.Tensor(0.030352738, shape=(), dtype=float32)\n",
            "Epoch  1950 Loss =  tf.Tensor(0.0136696035, shape=(), dtype=float32)\n",
            "Epoch  2000 Loss =  tf.Tensor(0.0075912634, shape=(), dtype=float32)\n",
            "Epoch  2050 Loss =  tf.Tensor(0.0054897186, shape=(), dtype=float32)\n",
            "Epoch  2100 Loss =  tf.Tensor(0.004452293, shape=(), dtype=float32)\n",
            "Epoch  2150 Loss =  tf.Tensor(0.004574257, shape=(), dtype=float32)\n",
            "Epoch  2200 Loss =  tf.Tensor(0.0038191956, shape=(), dtype=float32)\n",
            "Epoch  2250 Loss =  tf.Tensor(0.0025666263, shape=(), dtype=float32)\n",
            "Epoch  2300 Loss =  tf.Tensor(0.0021209486, shape=(), dtype=float32)\n",
            "Epoch  2350 Loss =  tf.Tensor(0.023593552, shape=(), dtype=float32)\n",
            "Epoch  2400 Loss =  tf.Tensor(0.01876486, shape=(), dtype=float32)\n",
            "Epoch  2450 Loss =  tf.Tensor(0.018451542, shape=(), dtype=float32)\n",
            "Epoch  2500 Loss =  tf.Tensor(0.019780284, shape=(), dtype=float32)\n",
            "Epoch  2550 Loss =  tf.Tensor(0.022425562, shape=(), dtype=float32)\n",
            "Epoch  2600 Loss =  tf.Tensor(0.022206333, shape=(), dtype=float32)\n",
            "Epoch  2650 Loss =  tf.Tensor(0.018070754, shape=(), dtype=float32)\n",
            "Epoch  2700 Loss =  tf.Tensor(0.015232726, shape=(), dtype=float32)\n",
            "Epoch  2750 Loss =  tf.Tensor(0.013273779, shape=(), dtype=float32)\n",
            "Epoch  2800 Loss =  tf.Tensor(0.012180863, shape=(), dtype=float32)\n",
            "Epoch  2850 Loss =  tf.Tensor(0.010571339, shape=(), dtype=float32)\n",
            "Epoch  2900 Loss =  tf.Tensor(0.008622908, shape=(), dtype=float32)\n",
            "Epoch  2950 Loss =  tf.Tensor(0.008113178, shape=(), dtype=float32)\n",
            "Epoch  3000 Loss =  tf.Tensor(0.007410936, shape=(), dtype=float32)\n",
            "Epoch  3050 Loss =  tf.Tensor(0.006558928, shape=(), dtype=float32)\n",
            "Epoch  3100 Loss =  tf.Tensor(0.0058437344, shape=(), dtype=float32)\n",
            "Epoch  3150 Loss =  tf.Tensor(0.005264601, shape=(), dtype=float32)\n",
            "Epoch  3200 Loss =  tf.Tensor(0.004649001, shape=(), dtype=float32)\n",
            "Epoch  3250 Loss =  tf.Tensor(0.004244423, shape=(), dtype=float32)\n",
            "Epoch  3300 Loss =  tf.Tensor(0.003837201, shape=(), dtype=float32)\n",
            "Epoch  3350 Loss =  tf.Tensor(0.0034384686, shape=(), dtype=float32)\n",
            "Epoch  3400 Loss =  tf.Tensor(0.0030676154, shape=(), dtype=float32)\n",
            "Epoch  3450 Loss =  tf.Tensor(0.002718627, shape=(), dtype=float32)\n",
            "Epoch  3500 Loss =  tf.Tensor(0.0023945256, shape=(), dtype=float32)\n",
            "Epoch  3550 Loss =  tf.Tensor(0.002292037, shape=(), dtype=float32)\n",
            "Epoch  3600 Loss =  tf.Tensor(0.00203945, shape=(), dtype=float32)\n",
            "Epoch  3650 Loss =  tf.Tensor(0.0016911781, shape=(), dtype=float32)\n",
            "Epoch  3700 Loss =  tf.Tensor(0.0015403888, shape=(), dtype=float32)\n",
            "Epoch  3750 Loss =  tf.Tensor(0.001369154, shape=(), dtype=float32)\n",
            "Epoch  3800 Loss =  tf.Tensor(0.0012108475, shape=(), dtype=float32)\n",
            "Epoch  3850 Loss =  tf.Tensor(0.0010709823, shape=(), dtype=float32)\n",
            "Epoch  3900 Loss =  tf.Tensor(0.00093326386, shape=(), dtype=float32)\n",
            "Epoch  3950 Loss =  tf.Tensor(0.00082686206, shape=(), dtype=float32)\n",
            "Epoch  4000 Loss =  tf.Tensor(0.00074173644, shape=(), dtype=float32)\n",
            "Epoch  4050 Loss =  tf.Tensor(0.0006759204, shape=(), dtype=float32)\n",
            "Epoch  4100 Loss =  tf.Tensor(0.0007143369, shape=(), dtype=float32)\n",
            "Epoch  4150 Loss =  tf.Tensor(0.0007589022, shape=(), dtype=float32)\n",
            "Epoch  4200 Loss =  tf.Tensor(0.00068411103, shape=(), dtype=float32)\n",
            "Epoch  4250 Loss =  tf.Tensor(0.00061091664, shape=(), dtype=float32)\n",
            "Epoch  4300 Loss =  tf.Tensor(0.00055105874, shape=(), dtype=float32)\n",
            "Epoch  4350 Loss =  tf.Tensor(0.00048287679, shape=(), dtype=float32)\n",
            "Epoch  4400 Loss =  tf.Tensor(0.00041403167, shape=(), dtype=float32)\n",
            "Epoch  4450 Loss =  tf.Tensor(0.00043147293, shape=(), dtype=float32)\n",
            "Epoch  4500 Loss =  tf.Tensor(0.00049427495, shape=(), dtype=float32)\n",
            "Epoch  4550 Loss =  tf.Tensor(0.0004407006, shape=(), dtype=float32)\n",
            "Epoch  4600 Loss =  tf.Tensor(0.00037679862, shape=(), dtype=float32)\n",
            "Epoch  4650 Loss =  tf.Tensor(0.00038753217, shape=(), dtype=float32)\n",
            "Epoch  4700 Loss =  tf.Tensor(0.0003991894, shape=(), dtype=float32)\n",
            "Epoch  4750 Loss =  tf.Tensor(0.00048546414, shape=(), dtype=float32)\n",
            "Epoch  4800 Loss =  tf.Tensor(0.0005659908, shape=(), dtype=float32)\n",
            "Epoch  4850 Loss =  tf.Tensor(0.00044694715, shape=(), dtype=float32)\n",
            "Epoch  4900 Loss =  tf.Tensor(0.00039315346, shape=(), dtype=float32)\n",
            "Epoch  4950 Loss =  tf.Tensor(0.00037920635, shape=(), dtype=float32)\n",
            "Epoch  5000 Loss =  tf.Tensor(0.0004096094, shape=(), dtype=float32)\n",
            "Epoch  5050 Loss =  tf.Tensor(0.00050857215, shape=(), dtype=float32)\n",
            "Epoch  5100 Loss =  tf.Tensor(0.0015442725, shape=(), dtype=float32)\n",
            "Epoch  5150 Loss =  tf.Tensor(0.00033924478, shape=(), dtype=float32)\n",
            "Epoch  5200 Loss =  tf.Tensor(0.00033380807, shape=(), dtype=float32)\n",
            "Epoch  5250 Loss =  tf.Tensor(0.00033091914, shape=(), dtype=float32)\n",
            "Epoch  5300 Loss =  tf.Tensor(0.00032961293, shape=(), dtype=float32)\n",
            "Epoch  5350 Loss =  tf.Tensor(0.0003301696, shape=(), dtype=float32)\n",
            "Epoch  5400 Loss =  tf.Tensor(0.00033372958, shape=(), dtype=float32)\n",
            "Epoch  5450 Loss =  tf.Tensor(0.00034534943, shape=(), dtype=float32)\n",
            "Epoch  5500 Loss =  tf.Tensor(0.00033736796, shape=(), dtype=float32)\n",
            "Epoch  5550 Loss =  tf.Tensor(0.0003426047, shape=(), dtype=float32)\n",
            "Epoch  5600 Loss =  tf.Tensor(0.00036760286, shape=(), dtype=float32)\n",
            "Epoch  5650 Loss =  tf.Tensor(0.00035943356, shape=(), dtype=float32)\n",
            "Epoch  5700 Loss =  tf.Tensor(0.0003693433, shape=(), dtype=float32)\n",
            "Epoch  5750 Loss =  tf.Tensor(0.00039031747, shape=(), dtype=float32)\n",
            "Epoch  5800 Loss =  tf.Tensor(0.0014171717, shape=(), dtype=float32)\n",
            "Epoch  5850 Loss =  tf.Tensor(0.06200702, shape=(), dtype=float32)\n",
            "Epoch  5900 Loss =  tf.Tensor(0.05391544, shape=(), dtype=float32)\n",
            "Epoch  5950 Loss =  tf.Tensor(0.053225342, shape=(), dtype=float32)\n",
            "Epoch  6000 Loss =  tf.Tensor(0.052888077, shape=(), dtype=float32)\n",
            "Epoch  6050 Loss =  tf.Tensor(0.052784275, shape=(), dtype=float32)\n",
            "Epoch  6100 Loss =  tf.Tensor(0.052649304, shape=(), dtype=float32)\n",
            "Epoch  6150 Loss =  tf.Tensor(0.052490488, shape=(), dtype=float32)\n",
            "Epoch  6200 Loss =  tf.Tensor(0.052340932, shape=(), dtype=float32)\n",
            "Epoch  6250 Loss =  tf.Tensor(0.052204724, shape=(), dtype=float32)\n",
            "Epoch  6300 Loss =  tf.Tensor(0.05207908, shape=(), dtype=float32)\n",
            "Epoch  6350 Loss =  tf.Tensor(0.051965483, shape=(), dtype=float32)\n",
            "Epoch  6400 Loss =  tf.Tensor(0.051860422, shape=(), dtype=float32)\n",
            "Epoch  6450 Loss =  tf.Tensor(0.051755745, shape=(), dtype=float32)\n",
            "Epoch  6500 Loss =  tf.Tensor(0.051629417, shape=(), dtype=float32)\n",
            "Epoch  6550 Loss =  tf.Tensor(0.051498286, shape=(), dtype=float32)\n",
            "Epoch  6600 Loss =  tf.Tensor(0.051365927, shape=(), dtype=float32)\n",
            "Epoch  6650 Loss =  tf.Tensor(0.05123733, shape=(), dtype=float32)\n",
            "Epoch  6700 Loss =  tf.Tensor(0.05111811, shape=(), dtype=float32)\n",
            "Epoch  6750 Loss =  tf.Tensor(0.051023856, shape=(), dtype=float32)\n",
            "Epoch  6800 Loss =  tf.Tensor(0.050948285, shape=(), dtype=float32)\n",
            "Epoch  6850 Loss =  tf.Tensor(0.050889447, shape=(), dtype=float32)\n",
            "Epoch  6900 Loss =  tf.Tensor(0.05084566, shape=(), dtype=float32)\n",
            "Epoch  6950 Loss =  tf.Tensor(0.05076479, shape=(), dtype=float32)\n",
            "Epoch  7000 Loss =  tf.Tensor(0.05075523, shape=(), dtype=float32)\n",
            "Epoch  7050 Loss =  tf.Tensor(0.05068285, shape=(), dtype=float32)\n",
            "Epoch  7100 Loss =  tf.Tensor(0.05070189, shape=(), dtype=float32)\n",
            "Epoch  7150 Loss =  tf.Tensor(0.11856036, shape=(), dtype=float32)\n",
            "Epoch  7200 Loss =  tf.Tensor(0.23480654, shape=(), dtype=float32)\n",
            "Epoch  7250 Loss =  tf.Tensor(0.017309751, shape=(), dtype=float32)\n",
            "Epoch  7300 Loss =  tf.Tensor(0.0075382763, shape=(), dtype=float32)\n",
            "Epoch  7350 Loss =  tf.Tensor(0.0051622055, shape=(), dtype=float32)\n",
            "Epoch  7400 Loss =  tf.Tensor(0.004002799, shape=(), dtype=float32)\n",
            "Epoch  7450 Loss =  tf.Tensor(0.0033604433, shape=(), dtype=float32)\n",
            "Epoch  7500 Loss =  tf.Tensor(0.0030345072, shape=(), dtype=float32)\n",
            "Epoch  7550 Loss =  tf.Tensor(0.0028364784, shape=(), dtype=float32)\n",
            "Epoch  7600 Loss =  tf.Tensor(0.0027161392, shape=(), dtype=float32)\n",
            "Epoch  7650 Loss =  tf.Tensor(0.0026576612, shape=(), dtype=float32)\n",
            "Epoch  7700 Loss =  tf.Tensor(0.0026243664, shape=(), dtype=float32)\n",
            "Epoch  7750 Loss =  tf.Tensor(0.002604559, shape=(), dtype=float32)\n",
            "Epoch  7800 Loss =  tf.Tensor(0.0025924384, shape=(), dtype=float32)\n",
            "Epoch  7850 Loss =  tf.Tensor(0.002585065, shape=(), dtype=float32)\n",
            "Epoch  7900 Loss =  tf.Tensor(0.0025808942, shape=(), dtype=float32)\n",
            "Epoch  7950 Loss =  tf.Tensor(0.0025791484, shape=(), dtype=float32)\n",
            "Epoch  8000 Loss =  tf.Tensor(0.0025794317, shape=(), dtype=float32)\n",
            "Epoch  8050 Loss =  tf.Tensor(0.0025815754, shape=(), dtype=float32)\n",
            "Epoch  8100 Loss =  tf.Tensor(0.0025856174, shape=(), dtype=float32)\n",
            "Epoch  8150 Loss =  tf.Tensor(0.0025915548, shape=(), dtype=float32)\n",
            "Epoch  8200 Loss =  tf.Tensor(0.0025995187, shape=(), dtype=float32)\n",
            "Epoch  8250 Loss =  tf.Tensor(0.0026096767, shape=(), dtype=float32)\n",
            "Epoch  8300 Loss =  tf.Tensor(0.0026222111, shape=(), dtype=float32)\n",
            "Epoch  8350 Loss =  tf.Tensor(0.002637269, shape=(), dtype=float32)\n",
            "Epoch  8400 Loss =  tf.Tensor(0.0026550759, shape=(), dtype=float32)\n",
            "Epoch  8450 Loss =  tf.Tensor(0.0026758448, shape=(), dtype=float32)\n",
            "Epoch  8500 Loss =  tf.Tensor(0.0026997784, shape=(), dtype=float32)\n",
            "Epoch  8550 Loss =  tf.Tensor(0.0027271488, shape=(), dtype=float32)\n",
            "Epoch  8600 Loss =  tf.Tensor(0.002758216, shape=(), dtype=float32)\n",
            "Epoch  8650 Loss =  tf.Tensor(0.0027932439, shape=(), dtype=float32)\n",
            "Epoch  8700 Loss =  tf.Tensor(0.0028325384, shape=(), dtype=float32)\n",
            "Epoch  8750 Loss =  tf.Tensor(0.0028764284, shape=(), dtype=float32)\n",
            "Epoch  8800 Loss =  tf.Tensor(0.0029240614, shape=(), dtype=float32)\n",
            "Epoch  8850 Loss =  tf.Tensor(0.002943414, shape=(), dtype=float32)\n",
            "Epoch  8900 Loss =  tf.Tensor(0.002941968, shape=(), dtype=float32)\n",
            "Epoch  8950 Loss =  tf.Tensor(0.0030100313, shape=(), dtype=float32)\n",
            "Epoch  9000 Loss =  tf.Tensor(0.0031092362, shape=(), dtype=float32)\n",
            "Epoch  9050 Loss =  tf.Tensor(0.0032149656, shape=(), dtype=float32)\n",
            "Epoch  9100 Loss =  tf.Tensor(0.003329108, shape=(), dtype=float32)\n",
            "Epoch  9150 Loss =  tf.Tensor(0.0034543634, shape=(), dtype=float32)\n",
            "Epoch  9200 Loss =  tf.Tensor(0.003593596, shape=(), dtype=float32)\n",
            "Epoch  9250 Loss =  tf.Tensor(0.0037500907, shape=(), dtype=float32)\n",
            "Epoch  9300 Loss =  tf.Tensor(0.0039277463, shape=(), dtype=float32)\n",
            "Epoch  9350 Loss =  tf.Tensor(0.0041313157, shape=(), dtype=float32)\n",
            "Epoch  9400 Loss =  tf.Tensor(0.004367044, shape=(), dtype=float32)\n",
            "Epoch  9450 Loss =  tf.Tensor(0.004643072, shape=(), dtype=float32)\n",
            "Epoch  9500 Loss =  tf.Tensor(0.0049714404, shape=(), dtype=float32)\n",
            "Epoch  9550 Loss =  tf.Tensor(0.00534597, shape=(), dtype=float32)\n",
            "Epoch  9600 Loss =  tf.Tensor(0.0057403655, shape=(), dtype=float32)\n",
            "Epoch  9650 Loss =  tf.Tensor(0.0062754643, shape=(), dtype=float32)\n",
            "Epoch  9700 Loss =  tf.Tensor(0.0070290123, shape=(), dtype=float32)\n",
            "Epoch  9750 Loss =  tf.Tensor(0.0082256645, shape=(), dtype=float32)\n",
            "Epoch  9800 Loss =  tf.Tensor(0.011003924, shape=(), dtype=float32)\n",
            "Epoch  9850 Loss =  tf.Tensor(0.02977983, shape=(), dtype=float32)\n",
            "Epoch  9900 Loss =  tf.Tensor(0.032675322, shape=(), dtype=float32)\n",
            "Epoch  9950 Loss =  tf.Tensor(3.903585, shape=(), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[1.0000000e+00 2.1223242e-04]]\n",
            "\n",
            " [[0.0000000e+00 8.0056036e-01]]\n",
            "\n",
            " [[0.0000000e+00 1.7957294e+00]]\n",
            "\n",
            " [[0.0000000e+00 3.0923834e+00]]], shape=(4, 1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqbsEJEnWOc7"
      },
      "source": [
        "# Check result, generalisation\n",
        "In this part, I try to use the same network in another graph, to see if it can be used in different graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LDTXF5knmGv",
        "outputId": "a0774173-f48a-4aca-8a39-1f18f7e41450"
      },
      "source": [
        "feature_validation = tf.constant([\n",
        "                     [1], \n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0],\n",
        "                     [0]\n",
        "                    ], dtype=tf.float32)\n",
        "output_validation = tf.constant([\n",
        "                     [100.0], \n",
        "                     [100.0],\n",
        "                     [100.0],\n",
        "                     [100.0],\n",
        "                     [100.0]\n",
        "                    ], dtype=tf.float32)\n",
        "n = 10000\n",
        "input_validation = tf.concat([feature_validation, output_validation], axis = 1)\n",
        "input_validation = input_validation[:,tf.newaxis, :]\n",
        "neigh_validation = tf.constant([\n",
        "                     [n, 1, n, n, n], \n",
        "                     [1, n, 1, n, n],\n",
        "                     [n, 1, n, 1, n],\n",
        "                     [n, n, 1, n, 1],\n",
        "                     [n, n, n, 1, n],\n",
        "                    ], dtype=tf.float32)\n",
        "\n",
        "change_model = create_model(input_validation.shape, instantiate_layers())\n",
        "change_model.set_weights(model.get_weights())\n",
        "\n",
        "result_validation = input_validation\n",
        "model.reset_states()\n",
        "for i in range(20):\n",
        "  result_validation = forward(result_validation, neigh_validation, feature_validation, change_model, False)\n",
        "print(result_validation)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1.0000000e+00 2.1223242e-04]]\n",
            "\n",
            " [[0.0000000e+00 8.0056036e-01]]\n",
            "\n",
            " [[0.0000000e+00 1.7957294e+00]]\n",
            "\n",
            " [[0.0000000e+00 3.0923834e+00]]\n",
            "\n",
            " [[0.0000000e+00 3.5092049e+00]]], shape=(5, 1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}